{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Task 2 Report\n",
    "\n",
    "AAU Machine Learning  \n",
    "Author: Junel R.S. Solis\n",
    "\n",
    "# Introduction\n",
    "\n",
    "The main goal of this task was to perform a sentiment analysis on a large dataset of tweets using two different machine learning models. Selection of the models to be used is up to the creator to choose from. They should be tuned to get the best output and their performance should be compared.\n",
    "\n",
    "The goal of tweet sentiment analysis is, given the contents of a tweet, to predict whether it has a **positive** or a **negative** sentiment. The models presented in this task should be able to predict any given tweet's sentiment with a reasonable amount of accuracy.\n",
    "\n",
    "In this exercise, two models have been selected:\n",
    "\n",
    "- Support Vector Machine (SVM) with term frequency - inverse document frequency (TF-IDF) vectorization\n",
    "- Long Short-Term Memory (LSTM) recurrent neural network combined with GloVe Twitter vectors.\n",
    "\n",
    "# Data Processing\n",
    "\n",
    "The dataset is a subset of the Sentiment140 containing 1.6 million samples. The actual dataset provided for this task contained a fraction of the total samples - 160,000. This dataset contained two columns: _tweet_text_ and its corresponding _sentiment_label_. The _tweet_text_ column contained the original tweet including hashtags, RTs, mentions, emojis, HTML code and URLs.\n",
    "\n",
    "### Cleaning\n",
    "\n",
    "There were some tweets that exceeded the traditional Twitter character limit of 140. While Twitter now allows tweets to be up to 280 characters, the decision was made nevertheless to drop rows exceeding 140 characters from the dataset. The sample size was randomly reduced to 70,000 samples to reduce processing and training times. The _sentiment_label_ rows were converted to either **[0,1]**\n",
    "\n",
    "Some tweets contained encoded HTML in the form of **`&amp;`** and **`&quot;`** which appeared quite significantly in a word cloud of both the positive and negative tweets. These were decoded into their actual-character strings **&** and **\"** respectively. Some experimentation was done with lemmatization and stemming of entire sentences, but these did not seem to have a positive effect in the validation performances of either model.\n",
    "\n",
    "Further cleaning of the **tweet_text** rows included the following:\n",
    "\n",
    "- Removal of punctuations and numbers\n",
    "- Conversion to lowercase\n",
    "- Removing multiple spaces\n",
    "- Removal of words longer than 41 characters\n",
    "\n",
    "Spelling correction was attempted but eventually abandoned because of the memory requirements.\n",
    "\n",
    "### Vectorization\n",
    "\n",
    "Two vectorization methods were chosen selected:\n",
    "\n",
    "- Term frequency - inverse document frequency (TF-IDF) vectorization\n",
    "- Global Vectors for Word Representation (GloVe)\n",
    "\n",
    "TF-IDF is commonly used in natural language processing (NLP) workflows while GloVe is an unsupervised learning algorithm that provides already vectorized data against which a dataset can be compared.\n",
    "\n",
    "# Modelling\n",
    "\n",
    "The choice to use an SVM classifier and an LSTM recurrent neural network was based on information in the course pages and Internet searches regarding twitter sentiment analysis and natural language processing.\n",
    "\n",
    "It was quite simple to vectorize the SVM dataset with 2000 maximum features and fit the model using the Keras API.\n",
    "\n",
    "While an SVM classifier is relatively simple, an LSTM network is much more complex in that its node layers not only pass information forward but also have mechanisms to provide recurrent feedback to previous layers.\n",
    "\n",
    "# Results\n",
    "\n",
    "# Discussion\n",
    "\n",
    "Lack of experience was a major bottleneck in accomplishing this task, but it was an opportunity to learn about natural language processing and how vectorization is required in order for text content to be fed into machine and deep learning algorithms. In this regard, a very significant portion of time was used in researching the two models used as well as the concepts surrounding text vectorization.\n",
    "\n",
    "Because of its nature, language data is highly variable and requires one to examine the raw dataset more fully in order to identify patterns early on that offer a possibility for correction or normalization. An example of this were the encoded HTML and the 140+ character\n",
    "\n",
    "# Conclusions\n",
    "\n",
    "The SVM classifier model's accuracy of ~76.6% was outperformed by the LSTM model with its ~83.5% accuracy.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}