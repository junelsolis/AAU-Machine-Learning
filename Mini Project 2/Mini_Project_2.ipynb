{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Mini Project 2",
      "provenance": [],
      "collapsed_sections": [
        "CHXVmswSXiJ-",
        "Er0O_nphXsaN"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit ('tf': conda)",
      "metadata": {
        "interpreter": {
          "hash": "8499c6d565a2a5634e137ac96601ac4826fe017b6f3ea2ed20b87ead738b5c36"
        }
      }
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    }
  },
  "cells": [
    {
      "source": [
        "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/junelsolis/AAU-Machine-Learning/HEAD)"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHXVmswSXiJ-"
      },
      "source": [
        "# Initial setup and data import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vb0d3bM_EmUi",
        "outputId": "398d3773-576f-4288-d697-8578e46de2d0"
      },
      "source": [
        "!pip install tweet-preprocessor nltk keras-tuner textblob ipywidgets\n",
        "!pip install tqdm>=4.9.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvT_XJ8AEkZk"
      },
      "source": [
        "# Make default library imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import preprocessor as p\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "FpDgEYIfEkZm",
        "outputId": "59dd0d9a-3452-4cc8-ba6b-407ec9786ddf"
      },
      "source": [
        "# Read the data from file\n",
        "data = pd.read_csv('Sentiment140.tenPercent.sample.tweets.tsv', delimiter='\\t')\n",
        "data.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "V5i_CXRhEkZn",
        "outputId": "deea6f62-f67d-4dab-83fd-1f04ff8cb86f"
      },
      "source": [
        "# Check for null values in the data\n",
        "# Plot label histogram\n",
        "print('Null values present in labels: ' + str(data['sentiment_label'].isnull().values.any()))\n",
        "print('Null values present in tweet text: ' + str(data['tweet_text'].isnull().values.any()))\n",
        "print()\n",
        "\n",
        "plt.title('Distribution of sentiment values')\n",
        "plt.bar(['0','4'], [len(data['sentiment_label'].where(data['sentiment_label'] == 0)), len(data['sentiment_label'].where(data['sentiment_label'] == 4))])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er0O_nphXsaN"
      },
      "source": [
        "# Preprocess tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDlY9m0aXWPq",
        "outputId": "0dfbfbfa-5517-407f-cb34-f6c3925b8901"
      },
      "source": [
        "# Import NLTK dependencies\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Spacy dependencies\n",
        "# import spacy\n",
        "# nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rouefPEsrskV",
        "outputId": "12390eb5-5685-4ded-c92f-4940760bc684"
      },
      "source": [
        "from textblob import TextBlob, Word\n",
        "from nltk.stem import LancasterStemmer\n",
        "from os import path\n",
        "# import multiprocessing as mp\n",
        "from pandarallel import pandarallel\n",
        "\n",
        "pandarallel.initialize(progress_bar = True)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Pandarallel will run on 8 workers.\nINFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXu39rKt32hb"
      },
      "source": [
        "### Text processing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zWsbN6m38Q6"
      },
      "source": [
        "def process_tweet(tweet):\n",
        "\n",
        "  clean_tweet = p.clean(tweet)\n",
        "  # Remove punctuations and numbers\n",
        "  clean_tweet = re.sub('[^a-zA-Z]', ' ', clean_tweet)\n",
        "\n",
        "  # Convert to lower case\n",
        "  clean_tweet = clean_tweet.lower()\n",
        "\n",
        "  # Single character removal\n",
        "  clean_tweet = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', clean_tweet)\n",
        "\n",
        "  # Removing multiple spaces\n",
        "  clean_tweet = re.sub(r'\\s+', ' ', clean_tweet)\n",
        "\n",
        "  # Remove words longer\n",
        "  clean_tweet_words = clean_tweet.split(' ')\n",
        "  filter_max_word_length_tweet = []\n",
        "  for w in clean_tweet_words:\n",
        "    if (len(w) <= 40):\n",
        "      filter_max_word_length_tweet.append(w)\n",
        "\n",
        "  clean_tweet =  \" \".join(filter_max_word_length_tweet)\n",
        "\n",
        "  # Remove multiple spaces again\n",
        "  clean_tweet = re.sub(r'\\s+', ' ', clean_tweet)\n",
        "  return clean_tweet\n",
        "\n",
        "\n",
        "tag_dict = {\"J\": 'a', \n",
        "            \"N\": 'n', \n",
        "            \"V\": 'v', \n",
        "            \"R\": 'r'}\n",
        "\n",
        "########################\n",
        "########################\n",
        "def lemmatize(tweet):\n",
        "  blob = TextBlob(tweet)\n",
        "\n",
        "  \n",
        "  words_and_tags = [(Word(w), tag_dict.get(pos[0], 'n')) for w, pos in blob.tags]\n",
        "  lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n",
        "\n",
        "  return \" \".join(lemmatized_list)\n",
        "\n",
        "\n",
        "########################\n",
        "########################\n",
        "def stem(tweet):\n",
        "  return\n",
        "\n",
        "########################\n",
        "########################\n",
        "def remove_stop_words(tweet):\n",
        "\n",
        "  word_tokens = word_tokenize(tweet)  \n",
        "  \n",
        "  filtered_sentence = []  \n",
        "    \n",
        "  for w in word_tokens:  \n",
        "      if w not in stop_words:  \n",
        "          filtered_sentence.append(w)  \n",
        "\n",
        "  return \" \".join(filtered_sentence)\n",
        "\n",
        "########################\n",
        "########################\n",
        "def correct_spelling(tweet):\n",
        "  \n",
        "    blob = TextBlob(tweet)\n",
        "    return str(blob.correct())\n",
        "\n",
        "\n",
        "########################\n",
        "########################\n",
        "def empty_single_word_tweets(tweet):\n",
        "    if (len(tweet.split(' ')) > 1):\n",
        "        return tweet\n",
        "    else:\n",
        "        return ''\n",
        "\n"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK3voncCEkZo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7247b57e-9cd7-4e9c-9d72-1a9c1911fa4d"
      },
      "source": [
        "# Clean the tweets. \n",
        "# Remove the following:\n",
        "# - URLS\n",
        "# - Hashtags\n",
        "# - Mentions\n",
        "# - Reserved words (RT, FAV)\n",
        "# - Emojis\n",
        "# - Smileys\n",
        "# - Numbers\n",
        "\n",
        "if path.exists('clean_data.pkl'):\n",
        "    clean_data = pd.read_pickle('clean_data.pkl')\n",
        "else:\n",
        "\n",
        "    # initialize progress bars for pandas operations\n",
        "    tqdm.pandas()\n",
        "\n",
        "    # Copy the dataset and retain the original data\n",
        "    clean_data = data.copy()\n",
        "    clean_data = clean_data.sample(70000) # reduce number of samples\n",
        "\n",
        "    # Convert labels to binary\n",
        "    clean_data.loc[clean_data['sentiment_label'] == 4, 'sentiment_label'] = 1\n",
        "\n",
        "    # Run tweet preprocessor for cleaning\n",
        "    clean_data['tweet_text'] = clean_data['tweet_text'].parallel_apply(lambda row: process_tweet(row))\n",
        "\n",
        "    # Lemmatize\n",
        "    clean_data['tweet_text'] = clean_data['tweet_text'].parallel_apply(lambda row: lemmatize(row))\n",
        "\n",
        "    # Remove stop words\n",
        "    clean_data['tweet_text'] = clean_data['tweet_text'].parallel_apply(lambda row: remove_stop_words(row))\n",
        "\n",
        "    # Remove tweets with only a single word\n",
        "    clean_data['tweet_text'] = clean_data['tweet_text'].parallel_apply(lambda row: empty_single_word_tweets(row))\n",
        "    clean_data.drop(clean_data[clean_data['tweet_text'] == ''].index, inplace=True)\n",
        "\n",
        "    # Correct spelling\n",
        "    clean_data['tweet_text'] = clean_data['tweet_text'].parallel_apply(lambda row: correct_spelling(row))\n",
        "\n",
        "    # Save cleaned data to pickle to save time later\n",
        "    clean_data.to_pickle('clean_data.pkl')\n",
        "\n",
        "\n"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=8750), Label(value='0 / 8750'))), …",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a6806e722d584f18ab0bc047474f8afa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=8750), Label(value='0 / 8750'))), …",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3591b4e3c76a4e3d90d8db2e8732f4a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=8750), Label(value='0 / 8750'))), …",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61ee7b75827c4349a42dec0a0b70bcb5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=8750), Label(value='0 / 8750'))), …",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b01104b149f44f82b9e3aeaf4f67f029"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=8385), Label(value='0 / 8385'))), …",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b431a30bc6246d6832a99c0dfd927a6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG52ExC8WHxr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "outputId": "f05521d8-e000-42cd-8030-4f5ab934a009"
      },
      "source": [
        "# Print random sample of cleaned tweets\n",
        "clean_data['tweet_text'].sample(5)"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "123348    whole new day know go happen donna go eat hung...\n",
              "36764                                 pp wont even play sad\n",
              "153183                             tomorrow productive feel\n",
              "127497    love new profile pick saw pick blow want scree...\n",
              "67292     jealousy anna beach stuck stupid internship mi...\n",
              "Name: tweet_text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asC2-ajYnXQm"
      },
      "source": [
        "# Support Vector Machine"
      ]
    },
    {
      "source": [
        "## Prepare the dataset"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsoXgasnnccd"
      },
      "source": [
        "# Split the dataset into training and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(clean_data['tweet_text'], clean_data['sentiment_label'], test_size=0.2)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOebQbV4Igi7"
      },
      "source": [
        "## Vectorize using TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8zot3Xx14-t"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tf_idf_vect = TfidfVectorizer(max_features=2000)\n",
        "tf_idf_vect.fit(clean_data['tweet_text'])\n",
        "\n",
        "X_train_tf_idf = tf_idf_vect.transform(X_train)\n",
        "X_test_tf_idf = tf_idf_vect.transform(X_test)\n"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected string or bytes-like object",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-ab0bdf61a2e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtf_idf_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtf_idf_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX_train_tf_idf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_idf_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1825\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1826\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1827\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1828\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1829\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[1;32m   1204\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtYqQjFyIrmT"
      },
      "source": [
        "## Fit data to SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hx71da3I7ULV",
        "outputId": "505f4926-56f9-4f42-d872-3d4b30decad2"
      },
      "source": [
        "from sklearn import model_selection, naive_bayes, svm\n",
        "\n",
        "\n",
        "# Classifier - Algorithm - SVM\n",
        "# fit the training dataset on the classifier\n",
        "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto', verbose=True)\n",
        "SVM.fit(X_train_tf_idf, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyrbctgyIx2A"
      },
      "source": [
        "## Measure accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFF8NI8yI5CC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "cbb8fdf9-0565-4d07-da69-854e873c8237"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "# predict the labels on validation dataset\n",
        "predictions_SVM = SVM.predict(X_test_tf_idf)\n",
        "# Use accuracy_score function to get the accuracy\n",
        "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, y_test)*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnINm1KZYB1s"
      },
      "source": [
        "# LSTM/RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c15JmcqWLKn1"
      },
      "source": [
        "# Tokenize the text corpus\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000, split=\" \")\n",
        "tokenizer.fit_on_texts(clean_data['tweet_text'].values)\n",
        "\n",
        "X = tokenizer.texts_to_sequences(clean_data['tweet_text'].values)\n",
        "X = pad_sequences(X) # padding our text vector so they all have the same length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcpFzYljKB98"
      },
      "source": [
        "# Split the dataset into training and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(X, clean_data['sentiment_label'], test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wovCQ9-mKdgt"
      },
      "source": [
        "### Configure hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4FZAkbUEkZp"
      },
      "source": [
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, LSTM, Embedding, Dropout, SpatialDropout1D\n",
        "from keras.losses import SparseCategoricalCrossentropy\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "def build_model(hp):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    hp_units_embed = hp.Int('units_embed', min_value = 32, max_value = 512, step = 32)\n",
        "    model.add(Embedding(5000, hp_units_embed, input_length=X.shape[1]))\n",
        "\n",
        "    hp_spatial_dropout = hp.Float('spatial_dropout', min_value = 0.1, max_value = 0.6)\n",
        "    model.add(SpatialDropout1D(hp_spatial_dropout))\n",
        "\n",
        "    hp_units_1 = hp.Int('units_1', min_value = 32, max_value = 512, step = 32)\n",
        "    model.add(LSTM(hp_units_1, return_sequences = True, dropout = 0.3, recurrent_dropout = 0.2))\n",
        "    \n",
        "    hp_units_2 = hp.Int('units_2', min_value = 32, max_value = 512, step = 32)\n",
        "    model.add(LSTM(hp_units_2, dropout=0.3, recurrent_dropout=0.2))\n",
        "    \n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    # Tune the learning rate for the optimizer \n",
        "    # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4])\n",
        "\n",
        "    optimizer = hp.Choice('optimizer', ['adam', 'sgd']) \n",
        "\n",
        "    model.compile(loss=SparseCategoricalCrossentropy(), optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5LrDp1oEkZq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eb53e56-9ddd-4cee-c8c9-fd2e2ac85741"
      },
      "source": [
        "import kerastuner as kt\n",
        "import tensorflow as tf\n",
        "import IPython\n",
        "\n",
        "lstm_tuner = kt.Hyperband(build_model,\n",
        "                     objective = 'val_accuracy', \n",
        "                     max_epochs = 5,\n",
        "                     factor = 3,\n",
        "                     directory = './',\n",
        "                     project_name = 'lstm_tuning')\n",
        "\n",
        "\n",
        "class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
        "  def on_train_end(*args, **kwargs):\n",
        "    IPython.display.clear_output(wait = True)\n",
        "\n",
        "lstm_tuner.search(X_train_lstm, y_train_lstm, epochs = 1, validation_split=0.3, callbacks = [ClearTrainingOutput()])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps = lstm_tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
        "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
        "is {best_hps.get('learning_rate')}.\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOmlRMyiRaND"
      },
      "source": [
        "# Compile and train model\n",
        "lstm_model = tuner.hypermodel.build(best_hps)\n",
        "history = lstm_model.fit(X_train_lstm, y_train_lstm, epochs = 6, validation_split=0.3)\n",
        "model.save('lstm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRqAnjrlRZu_"
      },
      "source": []
    }
  ]
}