{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Mini Project 2",
      "provenance": [],
      "collapsed_sections": [
        "CHXVmswSXiJ-",
        "Er0O_nphXsaN"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit ('tf': conda)",
      "metadata": {
        "interpreter": {
          "hash": "8499c6d565a2a5634e137ac96601ac4826fe017b6f3ea2ed20b87ead738b5c36"
        }
      }
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    }
  },
  "cells": [
    {
      "source": [
        "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/junelsolis/AAU-Machine-Learning/HEAD)"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHXVmswSXiJ-"
      },
      "source": [
        "# Initial setup and data import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vb0d3bM_EmUi",
        "outputId": "398d3773-576f-4288-d697-8578e46de2d0",
        "tags": [
          "outputPrepend"
        ]
      },
      "source": [
        "!pip install tweet-preprocessor nltk keras-tuner textblob ipywidgets wordcloud swifter keras sklearn tensorflow tqdm pv autocorrect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvT_XJ8AEkZk"
      },
      "source": [
        "# Make default library imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import preprocessor as p\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import swifter\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "FpDgEYIfEkZm",
        "outputId": "59dd0d9a-3452-4cc8-ba6b-407ec9786ddf"
      },
      "source": [
        "# Read the data from file\n",
        "data = pd.read_csv('Sentiment140.tenPercent.sample.tweets.tsv', delimiter='\\t')\n",
        "data.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "V5i_CXRhEkZn",
        "outputId": "deea6f62-f67d-4dab-83fd-1f04ff8cb86f"
      },
      "source": [
        "# Check for null values in the data\n",
        "# Plot label histogram\n",
        "print('Null values present in labels: ' + str(data['sentiment_label'].isnull().values.any()))\n",
        "print('Null values present in tweet text: ' + str(data['tweet_text'].isnull().values.any()))\n",
        "print()\n",
        "\n",
        "plt.title('Distribution of sentiment values')\n",
        "plt.bar(['0','4'], [len(data['sentiment_label'].where(data['sentiment_label'] == 0)), len(data['sentiment_label'].where(data['sentiment_label'] == 4))])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er0O_nphXsaN"
      },
      "source": [
        "# Preprocess tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDlY9m0aXWPq",
        "outputId": "0dfbfbfa-5517-407f-cb34-f6c3925b8901"
      },
      "source": [
        "# Import NLTK dependencies\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rouefPEsrskV",
        "outputId": "12390eb5-5685-4ded-c92f-4940760bc684"
      },
      "source": [
        "from textblob import TextBlob, Word\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from autocorrect import Speller\n",
        "from os import path\n",
        "import swifter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXu39rKt32hb"
      },
      "source": [
        "### Text processing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zWsbN6m38Q6"
      },
      "source": [
        "def process_tweet(tweet):\n",
        "\n",
        "  clean_tweet = tweet\n",
        "  # clean_tweet = p.clean(tweet)\n",
        "  \n",
        "  # Remove punctuations and numbers\n",
        "  clean_tweet = re.sub('[^a-zA-Z]', ' ', clean_tweet)\n",
        "\n",
        "  # Convert to lower case\n",
        "  clean_tweet = clean_tweet.lower()\n",
        "\n",
        "  # Single character removal\n",
        "  clean_tweet = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', clean_tweet)\n",
        "\n",
        "  # Removing multiple spaces\n",
        "  clean_tweet = re.sub(r'\\s+', ' ', clean_tweet)\n",
        "\n",
        "  # Remove words longer than 41 chars\n",
        "  clean_tweet_words = clean_tweet.split(' ')\n",
        "  filter_max_word_length_tweet = []\n",
        "  for w in clean_tweet_words:\n",
        "    if (len(w) <= 40):\n",
        "      filter_max_word_length_tweet.append(w)\n",
        "\n",
        "  clean_tweet =  \" \".join(filter_max_word_length_tweet)\n",
        "\n",
        "  # Remove multiple spaces again\n",
        "  clean_tweet = re.sub(r'\\s+', ' ', clean_tweet)\n",
        "  return clean_tweet\n",
        "\n",
        "\n",
        "tag_dict = {\"J\": 'a', \n",
        "            \"N\": 'n', \n",
        "            \"V\": 'v', \n",
        "            \"R\": 'r'}\n",
        "\n",
        "########################\n",
        "########################\n",
        "def lemmatize(tweet):\n",
        "  blob = TextBlob(tweet)\n",
        "\n",
        "  \n",
        "  words_and_tags = [(Word(w), tag_dict.get(pos[0], 'n')) for w, pos in blob.tags]\n",
        "  lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n",
        "\n",
        "  return \" \".join(lemmatized_list)\n",
        "\n",
        "\n",
        "########################\n",
        "########################\n",
        "ps = PorterStemmer()\n",
        "\n",
        "def stem(tweet):\n",
        "  output = []\n",
        "  for w in tweet.split():\n",
        "      output.append(\"\".join(ps.stem(w)))\n",
        "\n",
        "  return \" \".join(output)\n",
        "\n",
        "########################\n",
        "########################\n",
        "def remove_stop_words(tweet):\n",
        "\n",
        "  word_tokens = word_tokenize(tweet)  \n",
        "  \n",
        "  filtered_sentence = []  \n",
        "    \n",
        "  for w in word_tokens:  \n",
        "      if w not in stop_words:  \n",
        "          filtered_sentence.append(w)  \n",
        "\n",
        "  return \" \".join(filtered_sentence)\n",
        "\n",
        "########################\n",
        "########################\n",
        "def correct_spelling(tweet):\n",
        "  \n",
        "    blob = TextBlob(tweet)\n",
        "    return str(blob.correct())\n",
        "\n",
        "\n",
        "########################\n",
        "########################\n",
        "def empty_single_word_tweets(tweet):\n",
        "    if (len(tweet.split(' ')) > 1):\n",
        "        return tweet\n",
        "    else:\n",
        "        return ''\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## IMPORTANT ##\n",
        "In this section, the tweet samples are preprocessed. As it takes a lot of time to do this, the pickled data has been saved to a file called __clean_data.pkl__\n",
        "\n",
        "If this file exists in the project directoy, it is automatically loaded and used for the rest of the notebook. If refreshing the data is needed, then delete the __clean_data.pkl__ file and run the cell below. __Be advised:__ it will take at least half an hour on regular PC's."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install autocorrect\n",
        "from autocorrect import Speller"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK3voncCEkZo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7247b57e-9cd7-4e9c-9d72-1a9c1911fa4d"
      },
      "source": [
        "# Clean the tweets. \n",
        "# Remove the following:\n",
        "# - URLS\n",
        "# - Hashtags\n",
        "# - Mentions\n",
        "# - Reserved words (RT, FAV)\n",
        "# - Emojis\n",
        "# - Smileys\n",
        "# - Numbers\n",
        "\n",
        "\n",
        "\n",
        "if path.exists('clean_data.pkl'):\n",
        "    clean_data = pd.read_pickle('clean_data.pkl')\n",
        "    \n",
        "else:\n",
        "\n",
        "    # Copy the dataset and retain the original data\n",
        "    clean_data = data.copy()\n",
        "    clean_data = clean_data.sample(70000) # reduce number of samples\n",
        "\n",
        "    # def clean_data.swifter.allow_dask_on_strings(enable=True)\n",
        "\n",
        "    # Convert labels to binary\n",
        "    clean_data.loc[clean_data['sentiment_label'] == 4, 'sentiment_label'] = 1\n",
        "\n",
        "\n",
        "    # Run initial clean with tweet-preprocessor\n",
        "    print('Initial cleaning...\\n')\n",
        "    clean_data['tweet_text'] = clean_data['tweet_text'].swifter.allow_dask_on_strings().apply(lambda row: p.clean(row))\n",
        "\n",
        "    # print('Spelling check...\\n')\n",
        "    # speller = Speller()\n",
        "    # n = 1000\n",
        "    # for g, df in clean_data.groupby(np.arange(len(clean_data)) // n):\n",
        "    #     df['tweet_text'] = df['tweet_text'].swifter.allow_dask_on_strings().apply(lambda row: speller(row))\n",
        "\n",
        "    # Lemmatize\n",
        "    # print('Lemmatizing...\\n')\n",
        "    # clean_data['tweet_text'] = clean_data['tweet_text'].swifter.allow_dask_on_strings().apply(lambda row: lemmatize(row))\n",
        "\n",
        "    # Stemming\n",
        "    # print('Stemming...\\n')\n",
        "    # clean_data['tweet_text'] = clean_data['tweet_text'].swifter.allow_dask_on_strings().apply(lambda row: stem(row))\n",
        "\n",
        "    # Run more cleaning\n",
        "    print('More cleaning...\\n')\n",
        "    clean_data['tweet_text'] = clean_data['tweet_text'].swifter.allow_dask_on_strings().apply(lambda row: process_tweet(row))\n",
        "\n",
        "    # Remove stop words\n",
        "    # print('Remove stop words...\\n')\n",
        "    # clean_data['tweet_text'] = clean_data['tweet_text'].swifter.allow_dask_on_strings().apply(lambda row: remove_stop_words(row))\n",
        "\n",
        "    # Correct spelling\n",
        "    # speller = Speller()\n",
        "    # print('Spelling check...\\n')\n",
        "    # clean_data['tweet_text'] = clean_data['tweet_text'].swifter.allow_dask_on_strings().apply(lambda row: speller(row))\n",
        "\n",
        "    # Remove tweets with only a single word\n",
        "    # print('Remove single-word tweets...\\n')\n",
        "    # clean_data['tweet_text'] = clean_data['tweet_text'].swifter.allow_dask_on_strings().apply(lambda row: empty_single_word_tweets(row))\n",
        "    # clean_data.drop(clean_data[clean_data['tweet_text'] == ''].index, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "    # Save cleaned data to pickle to save time later\n",
        "    clean_data.to_pickle('clean_data.pkl')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG52ExC8WHxr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "outputId": "f05521d8-e000-42cd-8030-4f5ab934a009"
      },
      "source": [
        "# Print random sample of cleaned tweets\n",
        "print(clean_data.count())\n",
        "clean_data.sample(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asC2-ajYnXQm"
      },
      "source": [
        "# Support Vector Machine"
      ]
    },
    {
      "source": [
        "## Prepare the dataset"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsoXgasnnccd"
      },
      "source": [
        "# # Split the dataset into training and test\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# X_train, X_test, y_train, y_test = train_test_split(clean_data['tweet_text'], clean_data['sentiment_label'], test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOebQbV4Igi7"
      },
      "source": [
        "## Vectorize using TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8zot3Xx14-t"
      },
      "source": [
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# tf_idf_vect = TfidfVectorizer(max_features=2000)\n",
        "# tf_idf_vect.fit(clean_data['tweet_text'])\n",
        "\n",
        "# X_train_tf_idf = tf_idf_vect.transform(X_train)\n",
        "# X_test_tf_idf = tf_idf_vect.transform(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtYqQjFyIrmT"
      },
      "source": [
        "## Fit data to SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hx71da3I7ULV",
        "outputId": "505f4926-56f9-4f42-d872-3d4b30decad2"
      },
      "source": [
        "# from sklearn import model_selection, naive_bayes, svm\n",
        "\n",
        "\n",
        "# # Classifier - Algorithm - SVM\n",
        "# # fit the training dataset on the classifier\n",
        "# SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto', verbose=True)\n",
        "# SVM.fit(X_train_tf_idf, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyrbctgyIx2A"
      },
      "source": [
        "## Measure accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFF8NI8yI5CC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "cbb8fdf9-0565-4d07-da69-854e873c8237"
      },
      "source": [
        "# from sklearn.metrics import accuracy_score\n",
        "# # predict the labels on validation dataset\n",
        "# predictions_SVM = SVM.predict(X_test_tf_idf)\n",
        "# # Use accuracy_score function to get the accuracy\n",
        "# print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, y_test)*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Use GloVe"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "### Download and extract GloVe\n",
        " - Do not download if the zip file already exists\n",
        " - Do not attempt extract if the __glove_data__ directory exists"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "if path.exists('glove.zip') == False:\n",
        "    url = \"http://nlp.stanford.edu/data/glove.twitter.27B.zip\" \n",
        "    response = requests.get(url, stream=True)\n",
        "    total_size_in_bytes= int(response.headers.get('content-length', 0))\n",
        "    block_size = 1024 #1 Kibibyte\n",
        "    progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
        "    with open('glove.zip', 'wb') as file:\n",
        "        for data in response.iter_content(block_size):\n",
        "            progress_bar.update(len(data))\n",
        "            file.write(data)\n",
        "    progress_bar.close()\n",
        "    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
        "        print(\"ERROR, something went wrong\")\n",
        "\n",
        "if path.exists('glove_data') == False:\n",
        "    n_files = !unzip -l glove.zip | grep . | wc -l\n",
        "    !unzip -o ./glove.zip -d ./glove_data/ | pv -l -s {n_files[0]} > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c15JmcqWLKn1"
      },
      "source": [
        "NB_WORDS = 2500\n",
        "GLOVE_DIM = 100\n",
        "\n",
        "# Tokenize the text corpus\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{\"}~\\t\\n', split=\" \")\n",
        "tokenizer.fit_on_texts(clean_data['tweet_text'].values)\n",
        "\n",
        "X = tokenizer.texts_to_sequences(clean_data['tweet_text'].values)\n",
        "X = pad_sequences(X) # padding our text vector so they all have the same length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "glove_file = 'glove.twitter.27B.' + str(GLOVE_DIM) + 'd.txt'\n",
        "emb_dict = {}\n",
        "glove = open('glove_data/' + glove_file)\n",
        "for line in glove:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vector = np.asarray(values[1:], dtype='float32')\n",
        "    emb_dict[word] = vector\n",
        "glove.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "emb_matrix = np.zeros((NB_WORDS, GLOVE_DIM))\n",
        "\n",
        "for w, i in tokenizer.word_index.items():\n",
        "    if i < NB_WORDS:\n",
        "        vect = emb_dict.get(w)\n",
        "        if vect is not None:\n",
        "            emb_matrix[i] = vect\n",
        "    else:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnINm1KZYB1s"
      },
      "source": [
        "# LSTM/RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcpFzYljKB98"
      },
      "source": [
        "# Split the dataset into training and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(X, clean_data['sentiment_label'], test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wovCQ9-mKdgt"
      },
      "source": [
        "### Configure hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4FZAkbUEkZp"
      },
      "source": [
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
        "from keras.losses import SparseCategoricalCrossentropy\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "def build_model(hp):\n",
        "\n",
        "    # variables to adjust during tuning\n",
        "    hp_units_1 = hp.Int('units_1', min_value = 32, max_value = 512, step = 32)\n",
        "    hp_units_2 = hp.Int('units_2', min_value = 32, max_value = 512, step = 32)\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4])\n",
        "    optimizer = hp.Choice('optimizer', ['adam', 'sgd']) \n",
        "\n",
        "    \n",
        "    # define model\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Embedding(NB_WORDS, GLOVE_DIM, input_length=X.shape[1]))\n",
        "    model.layers[0].set_weights([emb_matrix])\n",
        "    model.layers[0].trainable = False\n",
        "\n",
        "    model.add(LSTM(hp_units_1, return_sequences = True, dropout = 0.3, recurrent_dropout = 0.2))\n",
        "    model.add(LSTM(hp_units_2, dropout=0.3, recurrent_dropout=0.2))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    model.compile(loss=SparseCategoricalCrossentropy(), optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "### Hyperparameter tuning\n",
        "This cell executes hyperparameter tuning. On this se"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5LrDp1oEkZq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eb53e56-9ddd-4cee-c8c9-fd2e2ac85741"
      },
      "source": [
        "import kerastuner as kt\n",
        "import tensorflow as tf\n",
        "import IPython\n",
        "\n",
        "lstm_tuner = kt.Hyperband(build_model,\n",
        "                     objective = 'val_accuracy', \n",
        "                     max_epochs = 5,\n",
        "                     factor = 3,\n",
        "                     directory = './',\n",
        "                     project_name = 'lstm_tuning')\n",
        "\n",
        "\n",
        "class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
        "  def on_train_end(*args, **kwargs):\n",
        "    IPython.display.clear_output(wait = True)\n",
        "\n",
        "# lstm_tuner.search(X_train_lstm, y_train_lstm, epochs = 1, validation_split=0.3, callbacks = [ClearTrainingOutput()])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps = lstm_tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
        "best_hps\n",
        "\n",
        "# print(f\"\"\"\n",
        "# The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
        "# layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
        "# is {best_hps.get('learning_rate')}.\n",
        "# \"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "### Compile and train\n",
        "Load and use the hyperparameter values gathered during tuning to build the actual model and save to disk as __lstm__"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOmlRMyiRaND"
      },
      "source": [
        "# Compile and train model\n",
        "if os.path('models/lstm') == False:\n",
        "    lstm_model = lstm_tuner.hypermodel.build(best_hps)\n",
        "    history = lstm_model.fit(X_train_lstm, y_train_lstm, epochs = 6, validation_split=0.3)\n",
        "\n",
        "    with open('models/lstm_training_history', 'wb') as history_file:\n",
        "        pickle.dump(history.history, history_file)\n",
        "\n",
        "    lstm_model.save('lstm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRqAnjrlRZu_"
      },
      "source": []
    }
  ]
}