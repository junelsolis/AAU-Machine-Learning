<!DOCTYPE html>
<html>
<head>
<title>Mini_Project_2_Report.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="task-2-report">Task 2 Report</h1>
<p>AAU Machine Learning<br>
Author: Junel R.S. Solis</p>
<h1 id="introduction">Introduction</h1>
<p>The main goal of this task was to perform a sentiment analysis on a large dataset of tweets using two different machine learning models. Selection of the models to be used is up to the creator to choose from. They should be tuned to get the best output and their performance should be compared.</p>
<p>The goal of tweet sentiment analysis is, given the contents of a tweet, to predict whether it has a <strong>positive</strong> or a <strong>negative</strong> sentiment. The models presented in this task should be able to predict any given tweet's sentiment with a reasonable amount of accuracy.</p>
<p>In this exercise, two models have been selected:</p>
<ul>
<li>Support Vector Machine (SVM) with term frequency - inverse document frequency (TF-IDF) vectorization</li>
<li>Long Short-Term Memory (LSTM) recurrent neural network combined with GloVe Twitter vectors.</li>
</ul>
<h1 id="data-processing">Data Processing</h1>
<p>The dataset is a subset of the Sentiment140 dataset from Stanford University containing 1.6 million samples. The actual dataset provided for this task contained a fraction of the total samples - 160,000 exactly. This dataset contained two columns: <em>tweet_text</em> and its corresponding <em>sentiment_label</em>. The <em>tweet_text</em> column contained the original tweet including hashtags, RTs, mentions, emojis, HTML code and URLs.</p>
<h3 id="cleaning">Cleaning</h3>
<p>There were some tweets that exceeded the traditional Twitter character limit of 140. While Twitter now allows tweets to be up to 280 characters, the decision was made nevertheless to drop rows exceeding 140 characters from the dataset. The sample size was randomly reduced to 70,000 samples to reduce processing and training times. The <em>sentiment_label</em> rows were converted to either <strong>[0,1]</strong></p>
<p>Some tweets contained encoded HTML in the form of <strong><code>&amp;amp;</code></strong> and <strong><code>&amp;quot;</code></strong> which appeared very frequently in a word cloud of both the positive and negative tweets. These were decoded into their real-character strings <strong>&amp;</strong> and <strong>&quot;</strong> respectively. Some experimentation was done with lemmatization and stemming of entire sentences, but these did not seem to have a positive effect in the testing performance of either model and was eventually left out of the cleaning pipeline.</p>
<p>Further cleaning of the <strong>tweet_text</strong> rows included the following:</p>
<ul>
<li>Removal of punctuations and numbers</li>
<li>Conversion to lowercase</li>
<li>Removing multiple spaces</li>
<li>Removal of words longer than 41 characters</li>
</ul>
<p>Spelling correction was attempted but eventually abandoned because of the memory requirements.</p>
<h3 id="vectorization">Vectorization</h3>
<p>Two vectorization methods were chosen selected:</p>
<ul>
<li>Term frequency - inverse document frequency (TF-IDF) vectorization</li>
<li>Global Vectors for Word Representation (GloVe)</li>
</ul>
<p>TF-IDF is commonly used in natural language processing (NLP) workflows while GloVe is an unsupervised learning algorithm that provides already vectorized data against which a dataset can be compared.</p>
<h1 id="modelling">Modelling</h1>
<p>The choice to use an SVM classifier and an LSTM recurrent neural network was based on information in the course pages and Internet searches regarding twitter sentiment analysis and natural language processing.</p>
<p>It was quite simple to vectorize the SVM dataset with 2000 maximum features and fit the model using the Keras API.</p>
<p>While an SVM classifier is relatively simple, an LSTM network is much more complex in that its node layers not only pass information forward but also have mechanisms to provide recurrent feedback to previous layers. Hyperparameter tuning was also performed on the LSTM network using the Keras Tuner API in order to help derive optimum values for the number of units in the RNN layers and the learning rate. Attempts were made to perform automatic tuning of the dropout and recurrent dropout rates of the LSTM layers but these attempts yielded inferior performance and were eventually tuned manually.</p>
<h1 id="results">Results</h1>
<p>The SVM classifier model achieved an accuracy of ~76.6% with minimal time elapsed for fitting the data. In contrast, the accuracy of the LSTM model was significantly higher at ~84.9%. A graph comparing the model accuracies is shown below.</p>
<p><img src="model_accuracy.png" alt=""></p>
<p>During training it was also observed that validation loss started to increase and validation accuracy started to decrease around 6 epochs and therefore the training epochs was limited to this number. Note the quick loss increase after epoch 5. The model architecture is also shown.</p>
<p><img src="lstm_loss.png" alt=""><img src="lstm_accuracy.png" alt=""></p>
<pre class="hljs"><code><div>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
embedding (Embedding)        (None, 31, 200)           1000000
_________________________________________________________________
lstm (LSTM)                  (None, 31, 256)           467968
_________________________________________________________________
lstm_1 (LSTM)                (None, 288)               627840
_________________________________________________________________
dense (Dense)                (None, 2)                 578
=================================================================
Total params: 2,096,386
Trainable params: 2,096,386
Non-trainable params: 0
_________________________________________________________________
</div></code></pre>
<h2 id="predictions-with-some-example-tweets">Predictions with some example tweets</h2>
<p>The following are example tweets taken from official Twitter feeds and showcases a little the predictive power of both models trained during this task. A checkmark ✅ next to the sentiment indicates that the prediction is correct.</p>
<h3 id="realdonaldtrump">@realdonaldtrump</h3>
<blockquote>
<p>Crazy Joe Biden is trying to act like a tough guy. Actually, he is weak, both mentally and physically, and yet he threatens me, for the second time, with physical assault. He doesn’t know me, but he would go down fast and hard, crying all the way. Don’t threaten people Joe!</p>
</blockquote>
<ul>
<li>SVC Prediction: <strong>Negative</strong> ✅</li>
<li>LSTM Prediction: <strong>Negative</strong> ✅</li>
</ul>
<blockquote>
<p>Sorry losers and haters, but my I.Q. is one of the highest -and you all know it! Please don’t feel so stupid or insecure,it’s not your fault</p>
</blockquote>
<ul>
<li>SVC Prediction: <strong>Negative</strong> ✅</li>
<li>LSTM Prediction: <strong>Negative</strong> ✅</li>
</ul>
<h3 id="joebiden">@JoeBiden</h3>
<blockquote>
<p>President of the United States, husband to @DrBiden, proud father &amp; grandfather. Ready to build back better for all Americans.</p>
</blockquote>
<ul>
<li>SVC Prediction: <strong>Positive</strong> ✅</li>
<li>LSTM Prediction: <strong>Positive</strong> ✅</li>
</ul>
<h3 id="barackobama">@BarackObama</h3>
<blockquote>
<p>No one is born hating another person because of the color of his skin or his background or his religion...</p>
</blockquote>
<ul>
<li>SVC Prediction: <strong>Positive</strong> ✅</li>
<li>LSTM Prediction: <strong>Positive</strong> ✅</li>
</ul>
<blockquote>
<p>It's been the honor of my life to serve you. You made me a better leader and a better man.</p>
</blockquote>
<ul>
<li>SVC Prediction: <strong>Positive</strong> ✅</li>
<li>LSTM Prediction: <strong>Positive</strong> ✅</li>
</ul>
<h1 id="discussion">Discussion</h1>
<p>A lack of experience was a major bottleneck in accomplishing this task, but it was an opportunity to learn about natural language processing and how vectorization is required in order for text content to be fed into machine and deep learning algorithms. In this regard, a very significant portion of time was used in researching the two models used as well as the concepts surrounding text vectorization.</p>
<p>Because of its nature, language data is highly variable and requires one to examine the raw dataset more fully in order to identify patterns early on that offer a possibility for correction or normalization. An example of this were the encoded HTML present in the tweets. Decoding those HTML snippets resulted in a small increase in accuracy for both models.</p>
<p>One aspect that might have improved the accuracy of these models is to add spelling correction during the preprocessing phase. This was attempted a few times over the course of the task but I was unable to scale the available spelling correction libraries to such a large dataset. Perhaps a future improvement to the preprocessing phase should include splitting the dataset into smaller batches and performing the spelling checks on those batches so as to bypass the enormous memory requirements.</p>
<p>While it is a good to observe a model with high predictive power, it may not be sufficient to rely solely on the predictions of one model. It is even better when there is agreement between models because that increases the confidence in the predicted result.</p>
<h1 id="conclusions">Conclusions</h1>
<p>The SVM classifier model's accuracy of <strong>~76.6%</strong> was outperformed by the LSTM model with its <strong>~84.9%</strong> accuracy.</p>
<p>A conclusion may also be made that considerable amounts of time must be spent fine-tuning a model's parameters as well as in data preprocessing in order to get consistent and reliable predictions.</p>
<p>Overall, this was a very worthwhile exercise to perform as it encouraged a deeper look into natural language processing, text vectorization, recurrent neural networks and hyperparameter tuning using the Keras Tuner API.</p>

</body>
</html>
